# TPU Model Deployment Utilization & Optimization Toolkit

A small, practical repo for diagnosing TPU utilization and performance bottlenecks in ML model deployments. It ingests profiler artifacts (best effort), XLA compilation logs, and simple metrics CSVs, then outputs a ranked, actionable recommendation list plus charts.

This is not a full replacement for TensorBoard profiling. It is a lightweight analyzer that is easy to run locally and helps narrow down likely causes quickly.

## What this is
- A CLI (`tpuopt`) that analyzes profiling artifacts and produces `summary.json`, `recommendations.md`, and charts.
- A simple Streamlit dashboard for one-page views of utilization and recommendations.
- A small set of heuristics focused on input pipeline, compilation/recompiles, batch sizing, compute/idle ratios, and memory pressure signals.

## What this is not
- It does not require access to internal Google tools.
- It does not do low-level kernel analysis beyond what is exposed in profiler artifacts.
- It does not replace collecting a full TPU profile.

## Quick start (local)
```bash
python -m venv .venv
. .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -U pip
pip install .

tpuopt sample-data

tpuopt analyze \
  --profile_dir ./sample_data \
  --model_name demo-model \
  --workload inference \
  --out_dir ./outputs

# optional report
tpuopt report --input ./outputs/summary.json --out_dir ./report
```

Outputs:
- `./outputs/summary.json`
- `./outputs/recommendations.md`
- `./outputs/charts/` (Plotly HTML)

## Run on a TPU VM (GCP)
1. Collect TensorBoard profile artifacts:
   - Use `tf.profiler.experimental.start` or equivalent TPU profiler hooks.
   - Keep the profile directory for a stable window (e.g., 1–5 minutes).
2. Export metrics to CSV (or emit them directly from your trainer/serving loop).
3. Run the analyzer:
```bash
pip install .

tpuopt analyze \
  --profile_dir /path/to/tpu/profile \
  --model_name your-model \
  --workload training \
  --out_dir ./outputs
```

Optional: if `GOOGLE_APPLICATION_CREDENTIALS` is set, the tool will attempt to initialize GCP Monitoring clients. You must still configure the metric filters for your project; the default behavior is no-op.

## How to interpret results
- `recommendations.md` is a ranked list. Start at the top.
- Each entry includes the symptom, likely root cause, exact metric to check, and specific remediation steps.
- If the input artifacts are incomplete, the tool will say so in the Notes section of the report.

## Streamlit dashboard (local)
```bash
pip install .[streamlit]
streamlit run streamlit_app.py
```

## Deploy to Cloud Run (Streamlit)
This uses the provided `Dockerfile`, which runs the Streamlit dashboard on port `8080`.

```bash
# one-time setup
gcloud auth login
gcloud config set project YOUR_PROJECT_ID

# deploy from source
gcloud run deploy tpuopt-dashboard \
  --source . \
  --region us-central1 \
  --allow-unauthenticated
```

Notes:
- Cloud Run requires your container to listen on port `8080`.
- The `Dockerfile.cli` is available if you want a CLI-only container.

## Repository layout
- `src/tpuopt/`: core library and CLI
- `docs/`: architecture and tuning cheat sheet
- `examples/`: commands and expected outputs
- `sample_data/`: sample artifacts (generated by `tpuopt sample-data`)
- `sample_outputs/`: example output files for reviewers

## Notes / assumptions
- Recommendations are inferred from available metrics, not from direct kernel-level profiling.
- For best results, include a metrics CSV with step time breakdown and an XLA compilation log.
- The heuristics are intentionally simple so they can be reviewed and adapted.

## License
Apache-2.0
